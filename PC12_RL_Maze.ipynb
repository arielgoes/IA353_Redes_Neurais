{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMsNyA3WPrER"
      },
      "source": [
        "##**Notebook PC#12**\n",
        "\n",
        "## Solving a Maze with Deep Reinforcement Learning.\n",
        "\n",
        "# Code based on [this content](https://www.samyzaf.com/ML/rl/qmaze.html).\n",
        "\n",
        "**Professor:** Fernando J. Von Zuben <br>\n",
        "**Aluno(a):** Ariel Góes de Castro<br>\n",
        "**Aluno(a):** Francisco Germano Vogt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjhi5kdG4FgT"
      },
      "source": [
        "### Importações e definições"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-aDpGQDz4Fg0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from time import sleep\n",
        "from IPython import display\n",
        "import pylab as pl\n",
        "import os, sys, time, datetime, json, random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD , Adam, RMSprop\n",
        "from keras.layers import PReLU\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JQs4V1wY_jJS"
      },
      "outputs": [],
      "source": [
        "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
        "rat_mark = 0.5      # The current rat cell will be painted by gray 0.5\n",
        "LEFT = 0\n",
        "UP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "# Actions dictionary\n",
        "actions_dict = {\n",
        "    LEFT: 'left',\n",
        "    UP: 'up',\n",
        "    RIGHT: 'right',\n",
        "    DOWN: 'down',\n",
        "}\n",
        "\n",
        "num_actions = len(actions_dict)\n",
        "\n",
        "# Exploration factor\n",
        "epsilon = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d1gPgFfQAMu_"
      },
      "outputs": [],
      "source": [
        "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
        "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
        "# rat = (row, col) initial rat position (defaults to (0,0))\n",
        "\n",
        "class Qmaze(object):\n",
        "    def __init__(self, maze, rat=(0,0)):\n",
        "        self._maze = np.array(maze)\n",
        "        nrows, ncols = self._maze.shape\n",
        "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
        "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
        "        self.free_cells.remove(self.target)\n",
        "        if self._maze[self.target] == 0.0:\n",
        "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
        "        if not rat in self.free_cells:\n",
        "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
        "        self.reset(rat)\n",
        "\n",
        "    def reset(self, rat):\n",
        "        self.rat = rat\n",
        "        self.maze = np.copy(self._maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        row, col = rat\n",
        "        self.maze[row, col] = rat_mark\n",
        "        self.state = (row, col, 'start')\n",
        "        self.min_reward = -0.5 * self.maze.size\n",
        "        self.total_reward = 0\n",
        "        self.visited = set()\n",
        "\n",
        "    def update_state(self, action):\n",
        "        nrows, ncols = self.maze.shape\n",
        "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
        "\n",
        "        if self.maze[rat_row, rat_col] > 0.0:\n",
        "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
        "\n",
        "        valid_actions = self.valid_actions()\n",
        "\n",
        "        if not valid_actions:\n",
        "            nmode = 'blocked'\n",
        "        elif action in valid_actions:\n",
        "            nmode = 'valid'\n",
        "            if action == LEFT:\n",
        "                ncol -= 1\n",
        "            elif action == UP:\n",
        "                nrow -= 1\n",
        "            if action == RIGHT:\n",
        "                ncol += 1\n",
        "            elif action == DOWN:\n",
        "                nrow += 1\n",
        "        else:                  # invalid action, no change in rat position\n",
        "            mode = 'invalid'\n",
        "\n",
        "        # new state\n",
        "        self.state = (nrow, ncol, nmode)\n",
        "\n",
        "    def get_reward(self):\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 1.0\n",
        "        if mode == 'blocked':\n",
        "            return self.min_reward - 1\n",
        "        if (rat_row, rat_col) in self.visited:\n",
        "            return -0.25\n",
        "        if mode == 'invalid':\n",
        "            return -0.75\n",
        "        if mode == 'valid':\n",
        "            return -0.04\n",
        "\n",
        "    def act(self, action):\n",
        "        self.update_state(action)\n",
        "        reward = self.get_reward()\n",
        "        self.total_reward += reward\n",
        "        status = self.game_status()\n",
        "        envstate = self.observe()\n",
        "        return envstate, reward, status\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self.draw_env()\n",
        "        envstate = canvas.reshape((1, -1))\n",
        "        return envstate\n",
        "\n",
        "    def draw_env(self):\n",
        "        canvas = np.copy(self.maze)\n",
        "        nrows, ncols = self.maze.shape\n",
        "        # clear all visual marks\n",
        "        for r in range(nrows):\n",
        "            for c in range(ncols):\n",
        "                if canvas[r,c] > 0.0:\n",
        "                    canvas[r,c] = 1.0\n",
        "        # draw the rat\n",
        "        row, col, valid = self.state\n",
        "        canvas[row, col] = rat_mark\n",
        "        return canvas\n",
        "\n",
        "    def game_status(self):\n",
        "        if self.total_reward < self.min_reward:\n",
        "            return 'lose'\n",
        "        rat_row, rat_col, mode = self.state\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
        "            return 'win'\n",
        "\n",
        "        return 'not_over'\n",
        "\n",
        "    def valid_actions(self, cell=None):\n",
        "        if cell is None:\n",
        "            row, col, mode = self.state\n",
        "        else:\n",
        "            row, col = cell\n",
        "        actions = [0, 1, 2, 3]\n",
        "        nrows, ncols = self.maze.shape\n",
        "        if row == 0:\n",
        "            actions.remove(1)\n",
        "        elif row == nrows-1:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col == 0:\n",
        "            actions.remove(0)\n",
        "        elif col == ncols-1:\n",
        "            actions.remove(2)\n",
        "\n",
        "        if row>0 and self.maze[row-1,col] == 0.0:\n",
        "            actions.remove(1)\n",
        "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
        "            actions.remove(3)\n",
        "\n",
        "        if col>0 and self.maze[row,col-1] == 0.0:\n",
        "            actions.remove(0)\n",
        "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
        "            actions.remove(2)\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZEgvv0QHAcqO"
      },
      "outputs": [],
      "source": [
        "def show(qmaze):\n",
        "    plt.grid('on')\n",
        "    nrows, ncols = qmaze.maze.shape\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
        "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    canvas = np.copy(qmaze.maze)\n",
        "    for row,col in qmaze.visited:\n",
        "        canvas[row,col] = 0.6\n",
        "    rat_row, rat_col, _ = qmaze.state\n",
        "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
        "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
        "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gUWPPKi6B5tj"
      },
      "outputs": [],
      "source": [
        "maze =  np.array([\n",
        "    [ 1.,  0.,  1.,  1.,  1.],\n",
        "    [ 1.,  1.,  1.,  0.,  1.],\n",
        "    [ 0.,  0.,  0.,  1.,  1.],\n",
        "    [ 0.,  1.,  1.,  1.,  0.],\n",
        "    [ 1.,  1.,  0.,  1.,  1.]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTO46kedArMF"
      },
      "outputs": [],
      "source": [
        "qmaze = Qmaze(maze)\n",
        "canvas, reward, game_over = qmaze.act(DOWN)\n",
        "print(\"reward=\", reward)\n",
        "show(qmaze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QOkRblSA3IH"
      },
      "outputs": [],
      "source": [
        "qmaze.act(DOWN)  # move down\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(RIGHT)  # move right\n",
        "qmaze.act(UP)  # move up\n",
        "qmaze.act(RIGHT)  # move right\n",
        "show(qmaze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Qj1TKfnA9qu"
      },
      "outputs": [],
      "source": [
        "def play_game(model, qmaze, rat_cell):\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    while True:\n",
        "        prev_envstate = envstate\n",
        "        # get next action\n",
        "        q = model.predict(prev_envstate)\n",
        "        action = np.argmax(q[0])\n",
        "\n",
        "        # apply action, get rewards and new state\n",
        "        envstate, reward, game_status = qmaze.act(action)\n",
        "        if game_status == 'win':\n",
        "            return True\n",
        "        elif game_status == 'lose':\n",
        "            return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zw53DdHQBKF0"
      },
      "outputs": [],
      "source": [
        "def completion_check(model, qmaze):\n",
        "    for cell in qmaze.free_cells:\n",
        "        if not qmaze.valid_actions(cell):\n",
        "            return False\n",
        "        if not play_game(model, qmaze, cell):\n",
        "            return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bm9WstgwBgjg"
      },
      "outputs": [],
      "source": [
        "class Experience(object):\n",
        "    def __init__(self, model, max_memory=100, discount=0.95):\n",
        "        self.model = model\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "        self.num_actions = model.output_shape[-1]\n",
        "\n",
        "    def remember(self, episode):\n",
        "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
        "        # memory[i] = episode\n",
        "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
        "        self.memory.append(episode)\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def predict(self, envstate):\n",
        "        return self.model.predict(envstate)[0]\n",
        "\n",
        "    def get_data(self, data_size=10):\n",
        "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
        "        mem_size = len(self.memory)\n",
        "        data_size = min(mem_size, data_size)\n",
        "        inputs = np.zeros((data_size, env_size))\n",
        "        targets = np.zeros((data_size, self.num_actions))\n",
        "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
        "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
        "            inputs[i] = envstate\n",
        "            # There should be no target values for actions not taken.\n",
        "            targets[i] = self.predict(envstate)\n",
        "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
        "            Q_sa = np.max(self.predict(envstate_next))\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                # reward + gamma * max_a' Q(s', a')\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-QvK92JaBiL7"
      },
      "outputs": [],
      "source": [
        "def qtrain(model, maze, **opt):\n",
        "    global epsilon\n",
        "    n_epoch = opt[\"n_epoch\"]\n",
        "    max_memory = opt[\"max_memory\"]\n",
        "    data_size = opt[\"data_size\"]\n",
        "    # n_epoch = opt.get('n_epoch', 15000)\n",
        "    # max_memory = opt.get('max_memory', 1000)\n",
        "    # data_size = opt.get('data_size', 50)\n",
        "    weights_file = opt.get('weights_file', \"\")\n",
        "    name = opt.get('name', 'model')\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    # If you want to continue training from a previous model,\n",
        "    # just supply the h5 file name to weights_file option\n",
        "    if weights_file:\n",
        "        print(\"loading weights from file: %s\" % (weights_file,))\n",
        "        model.load_weights(weights_file)\n",
        "\n",
        "    # Construct environment/game from numpy array: maze (see above)\n",
        "    qmaze = Qmaze(maze)\n",
        "\n",
        "    # Initialize experience replay object\n",
        "    experience = Experience(model, max_memory=max_memory)\n",
        "\n",
        "    win_history = []   # history of win/lose game\n",
        "    n_free_cells = len(qmaze.free_cells)\n",
        "    hsize = qmaze.maze.size//2   # history window size\n",
        "    win_rate = 0.0\n",
        "    imctr = 1\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        loss = 0.0\n",
        "        rat_cell = random.choice(qmaze.free_cells)\n",
        "        qmaze.reset(rat_cell)\n",
        "        game_over = False\n",
        "\n",
        "        # get initial envstate (1d flattened canvas)\n",
        "        envstate = qmaze.observe()\n",
        "\n",
        "        n_episodes = 0\n",
        "        while not game_over:\n",
        "            valid_actions = qmaze.valid_actions()\n",
        "            if not valid_actions: break\n",
        "            prev_envstate = envstate\n",
        "            # Get next action\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = random.choice(valid_actions)\n",
        "            else:\n",
        "                action = np.argmax(experience.predict(prev_envstate))\n",
        "\n",
        "            # Apply action, get reward and new envstate\n",
        "            envstate, reward, game_status = qmaze.act(action)\n",
        "            if game_status == 'win':\n",
        "                win_history.append(1)\n",
        "                game_over = True\n",
        "            elif game_status == 'lose':\n",
        "                win_history.append(0)\n",
        "                game_over = True\n",
        "            else:\n",
        "                game_over = False\n",
        "\n",
        "            # Store episode (experience)\n",
        "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
        "            experience.remember(episode)\n",
        "            n_episodes += 1\n",
        "\n",
        "            # Train neural network model\n",
        "            inputs, targets = experience.get_data(data_size=data_size)\n",
        "            h = model.fit(\n",
        "                inputs,\n",
        "                targets,\n",
        "                epochs=8,\n",
        "                batch_size=16,\n",
        "                verbose=0,\n",
        "            )\n",
        "            loss = model.evaluate(inputs, targets, verbose=0)\n",
        "\n",
        "        if len(win_history) > hsize:\n",
        "            win_rate = sum(win_history[-hsize:]) / hsize\n",
        "\n",
        "        dt = datetime.datetime.now() - start_time\n",
        "        t = format_time(dt.total_seconds())\n",
        "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
        "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
        "        # we simply check if training has exhausted all free cells and if in all\n",
        "        # cases the agent won\n",
        "        if win_rate > 0.9 : epsilon = 0.05\n",
        "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
        "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
        "            break\n",
        "\n",
        "    # Save trained model weights and architecture, this will be used by the visualization code\n",
        "    h5file = name + \".h5\"\n",
        "    json_file = name + \".json\"\n",
        "    model.save_weights(h5file, overwrite=True)\n",
        "    with open(json_file, \"w\") as outfile:\n",
        "        json.dump(model.to_json(), outfile)\n",
        "    end_time = datetime.datetime.now()\n",
        "    dt = datetime.datetime.now() - start_time\n",
        "    seconds = dt.total_seconds()\n",
        "    t = format_time(seconds)\n",
        "    print('files: %s, %s' % (h5file, json_file))\n",
        "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
        "    return seconds\n",
        "\n",
        "# This is a small utility for printing readable time strings:\n",
        "def format_time(seconds):\n",
        "    if seconds < 400:\n",
        "        s = float(seconds)\n",
        "        return \"%.1f seconds\" % (s,)\n",
        "    elif seconds < 4000:\n",
        "        m = seconds / 60.0\n",
        "        return \"%.2f minutes\" % (m,)\n",
        "    else:\n",
        "        h = seconds / 3600.0\n",
        "        return \"%.2f hours\" % (h,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7W6tHgyyBzrr"
      },
      "outputs": [],
      "source": [
        "def build_model(maze, lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(maze.size))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dense(num_actions))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK9lCeTACAcy"
      },
      "outputs": [],
      "source": [
        "model = build_model(maze)\n",
        "qtrain(model, maze, n_epoch=50, max_memory=8*maze.size, data_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERXGZRE8rz7U"
      },
      "source": [
        "**Trajectory generated from a random point with the trained network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj3EC2GBDkYu"
      },
      "outputs": [],
      "source": [
        "rat_cell = random.choice(qmaze.free_cells)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "game_status = 'lose'\n",
        "while(game_status != 'win'):\n",
        "  q = model.predict(envstate)\n",
        "  action = np.argmax(q[0])\n",
        "  envstate, reward, game_status = qmaze.act(action)\n",
        "  show(qmaze)\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(pl.gcf())\n",
        "  plt.gca().clear()\n",
        "  sleep(0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w9ScypSsUhJ"
      },
      "outputs": [],
      "source": [
        "# Exhibition of the maximum Q value at each state of the environment\n",
        "directions = []\n",
        "for rat_cell in qmaze.free_cells:\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    q = model.predict(envstate)\n",
        "    action = np.argmax(q[0])\n",
        "    if action == 0:\n",
        "        directions.append('<b')\n",
        "    elif action == 1:\n",
        "        directions.append('^b')\n",
        "    elif action == 2:\n",
        "        directions.append('>b')\n",
        "    else:\n",
        "        directions.append('vb')\n",
        "\n",
        "rat_cell = (0,0)\n",
        "qmaze.reset(rat_cell)\n",
        "show(qmaze)\n",
        "k = 0\n",
        "for point in qmaze.free_cells:\n",
        "    plt.plot(point[1],point[0],directions[k])\n",
        "    k = k + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvFvasXVtWcC"
      },
      "outputs": [],
      "source": [
        "# Explicit Q values for some relevant states of the environment.\n",
        "for rat_cell in qmaze.free_cells:\n",
        "    qmaze.reset(rat_cell)\n",
        "    envstate = qmaze.observe()\n",
        "    q = model.predict(envstate)\n",
        "    action = np.argmax(q[0])\n",
        "    if action == 0:\n",
        "        directions.append('<b')\n",
        "    elif action == 1:\n",
        "        directions.append('^b')\n",
        "    elif action == 2:\n",
        "        directions.append('>b')\n",
        "    else:\n",
        "        directions.append('vb')\n",
        "\n",
        "plt.figure(0)\n",
        "rat_cell = (0,0)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "q0 = model.predict(envstate)\n",
        "show(qmaze)\n",
        "k = 0\n",
        "for point in qmaze.free_cells:\n",
        "    plt.plot(point[1],point[0],directions[k])\n",
        "    k = k + 1\n",
        "\n",
        "plt.figure(1)\n",
        "rat_cell = (3,1)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "q1 = model.predict(envstate)\n",
        "show(qmaze)\n",
        "k = 0\n",
        "for point in qmaze.free_cells:\n",
        "    plt.plot(point[1],point[0],directions[k])\n",
        "    k = k + 1\n",
        "\n",
        "plt.figure(2)\n",
        "rat_cell = (1,4)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "q2 = model.predict(envstate)\n",
        "show(qmaze)\n",
        "k = 0\n",
        "for point in qmaze.free_cells:\n",
        "    plt.plot(point[1],point[0],directions[k])\n",
        "    k = k + 1\n",
        "\n",
        "plt.figure(3)\n",
        "rat_cell = (3,3)\n",
        "qmaze.reset(rat_cell)\n",
        "envstate = qmaze.observe()\n",
        "q3 = model.predict(envstate)\n",
        "show(qmaze)\n",
        "k = 0\n",
        "for point in qmaze.free_cells:\n",
        "    plt.plot(point[1],point[0],directions[k])\n",
        "    k = k + 1\n",
        "\n",
        "print('Q-Valor (0,0)\\n< : {:6.3f}\\n^ : {:6.3f}\\n> : {:6.3f}\\nv : {:6.3f}\\n'.format(q0[0,0],q0[0,1],q0[0,2],q0[0,3]))\n",
        "print('Q-Valor (3,1)\\n< : {:6.3f}\\n^ : {:6.3f}\\n> : {:6.3f}\\nv : {:6.3f}\\n'.format(q1[0,0],q1[0,1],q1[0,2],q1[0,3]))\n",
        "print('Q-Valor (1,4)\\n< : {:6.3f}\\n^ : {:6.3f}\\n> : {:6.3f}\\nv : {:6.3f}\\n'.format(q2[0,0],q2[0,1],q2[0,2],q2[0,3]))\n",
        "print('Q-Valor (3,3)\\n< : {:6.3f}\\n^ : {:6.3f}\\n> : {:6.3f}\\nv : {:6.3f}'.format(q3[0,0],q3[0,1],q3[0,2],q3[0,3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mulXq9ZZKkX5"
      },
      "source": [
        "<font color=\"green\">\n",
        "Atividade (a) <br>\n",
        "Explique qual é a estratégia de punição e recompensa adotada.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGsvW_2eTpfB"
      },
      "source": [
        "**Resposta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZJYmLh-7ZNN"
      },
      "source": [
        "<font color=\"green\">\n",
        "Atividade (b) <br>\n",
        "Explique a relevância de 10% de ações aleatórias, associadas ao mecanismo de exploração implementado.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46RICs1m7ZNT"
      },
      "source": [
        "**Resposta:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EHmbE_5Ap1d"
      },
      "source": [
        "<font color=\"green\">\n",
        "Atividade (c) <br>\n",
        "Com base nas últimas 5 figuras produzidas com a execução do notebook, você pode afirmar que foi aprendida a política ótima de navegação no labirinto? Justifique.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OM3uwxTAp1d"
      },
      "source": [
        "**Resposta:**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
