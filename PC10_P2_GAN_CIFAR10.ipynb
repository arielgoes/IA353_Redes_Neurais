{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMsNyA3WPrER"
      },
      "source": [
        "##**Notebook PC#10 - Part 1**\n",
        "\n",
        "## Generative Adversarial Networks for the MNIST dataset.\n",
        "\n",
        "Based on [this content](https://towardsdatascience.com/generative-adversarial-networks-bf4e809180b3), with code corrections.\n",
        "\n",
        "**Professor:** Fernando J. Von Zuben <br>\n",
        "**Aluno(a):** Ariel GÃ³es de Castro <br>\n",
        "**Aluno(a):** Francisco Germano Vogt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-29 23:30:14.061559: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GAN():\n",
        "    def __init__(self, class_label):\n",
        "        self.img_rows = 32\n",
        "        self.img_cols = 32\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "        self.class_label = class_label\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256 * 8 * 8, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((8, 8, 256)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        (X_train, y_train), (_, _) = cifar10.load_data()\n",
        "\n",
        "        # Filter the dataset for the specified class label\n",
        "        X_train = X_train[y_train.flatten() == self.class_label]\n",
        "        \n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "\n",
        "        d_losses = []\n",
        "        d_accuracies = []\n",
        "        g_losses = []\n",
        "\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            d_losses.append(d_loss[0])\n",
        "            d_accuracies.append(100 * d_loss[1])\n",
        "            g_losses.append(g_loss)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "        return d_losses, d_accuracies, g_losses\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
        "                axs[i, j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"cifar10_class_%d_epoch_%d.png\" % (self.class_label, epoch))\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-29 23:30:16.342070: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2024-05-29 23:30:16.343210: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2024-05-29 23:30:16.358205: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2024-05-29 23:30:16.358229: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ariel-Nitro-AN515-44): /proc/driver/nvidia/version does not exist\n",
            "2024-05-29 23:30:16.358808: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-29 23:30:16.359818: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 16, 16, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 9, 9, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 5, 5, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 5, 5, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 5, 5, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 5, 256)         1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6401      \n",
            "=================================================================\n",
            "Total params: 396,609\n",
            "Trainable params: 395,713\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16384)             1654784   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 256)       1024      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 16, 256)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 32, 32, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 3)         3459      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 3)         0         \n",
            "=================================================================\n",
            "Total params: 2,544,899\n",
            "Trainable params: 2,544,131\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-29 23:30:17.208102: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2024-05-29 23:30:17.211440: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2894720000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 [D loss: 1.182238, acc.: 33.98%] [G loss: 0.668391]\n",
            "100 [D loss: 0.001844, acc.: 100.00%] [G loss: 0.067599]\n",
            "200 [D loss: 0.001041, acc.: 100.00%] [G loss: 0.068553]\n",
            "300 [D loss: 0.007705, acc.: 100.00%] [G loss: 0.243691]\n",
            "400 [D loss: 0.001500, acc.: 100.00%] [G loss: 0.086267]\n",
            "500 [D loss: 0.000195, acc.: 100.00%] [G loss: 0.168551]\n",
            "600 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.054163]\n",
            "700 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.044470]\n",
            "800 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.050589]\n",
            "900 [D loss: 0.000105, acc.: 100.00%] [G loss: 0.048611]\n",
            "1000 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.034474]\n",
            "1100 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.029668]\n",
            "1200 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.036412]\n",
            "1300 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.043002]\n",
            "1400 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.046770]\n",
            "1500 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.091421]\n",
            "1600 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.034860]\n",
            "1700 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.038181]\n",
            "1800 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.046758]\n",
            "1900 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.028355]\n",
            "2000 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.065359]\n",
            "2100 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.027115]\n",
            "2200 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.038974]\n",
            "2300 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.050045]\n",
            "2400 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.060566]\n",
            "2500 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.040931]\n",
            "2600 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.042883]\n",
            "2700 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.031517]\n",
            "2800 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.022310]\n",
            "2900 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.042700]\n",
            "3000 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.041212]\n",
            "3100 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.026493]\n",
            "3200 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.025939]\n",
            "3300 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.062494]\n",
            "3400 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.056438]\n",
            "3500 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.034854]\n",
            "3600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.031720]\n",
            "3700 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.046797]\n",
            "3800 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.038818]\n",
            "3900 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.041902]\n",
            "4000 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.025718]\n",
            "4100 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.036215]\n",
            "4200 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.033090]\n",
            "4300 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.029640]\n",
            "4400 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.026309]\n",
            "4500 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.035218]\n",
            "4600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.035573]\n",
            "4700 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.040464]\n",
            "4800 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.027979]\n",
            "4900 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.043033]\n",
            "5000 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.036540]\n",
            "5100 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.093151]\n",
            "5200 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.030926]\n",
            "5300 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.008667]\n",
            "5400 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.010305]\n",
            "5500 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.007449]\n",
            "5600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.005149]\n",
            "5700 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.001942]\n",
            "5800 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.003229]\n",
            "5900 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.004969]\n",
            "6000 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.002686]\n",
            "6100 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.004849]\n",
            "6200 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.002077]\n",
            "6300 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001918]\n",
            "6400 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.004433]\n",
            "6500 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.001012]\n",
            "6600 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000937]\n",
            "6700 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000780]\n",
            "6800 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.001013]\n",
            "6900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.001292]\n",
            "7000 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000459]\n",
            "7100 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.000745]\n",
            "7200 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000905]\n",
            "7300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000340]\n",
            "7400 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000549]\n",
            "7500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000362]\n",
            "7600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000111]\n",
            "7700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000253]\n",
            "7800 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.000255]\n",
            "7900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000494]\n",
            "8000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000250]\n",
            "8100 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000282]\n",
            "8200 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000170]\n",
            "8300 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000105]\n",
            "8400 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000102]\n",
            "8500 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000270]\n",
            "8600 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000456]\n",
            "8700 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000215]\n",
            "8800 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000254]\n",
            "8900 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000063]\n",
            "9000 [D loss: 0.000000, acc.: 100.00%] [G loss: 0.000077]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Example: Train the GAN on class 3 (cats) of CIFAR-10\n",
        "gan = GAN(class_label=3)\n",
        "d_losses, d_accuracies, g_losses = gan.train(epochs=30001, batch_size=128, sample_interval=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize losses\n",
        "plt.plot(d_losses, label='Discriminator')\n",
        "plt.plot(g_losses, label='Generator')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize accuracy\n",
        "plt.plot(d_accuracies)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy of the Discriminator')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_gpu_python-3-8",
      "language": "python",
      "name": "tf_gpu_python-3-8"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
